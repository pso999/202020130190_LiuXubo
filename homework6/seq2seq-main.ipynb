{
 "metadata": {
  "kernelspec": {
   "name": "mindspore-python3.7-aarch64",
   "display_name": "MindSpore-python3.7-aarch64",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1.导入实验所需的第三方模块"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import unicodedata  \n",
    "import math\n",
    "\n",
    "from mindspore import Tensor, nn, Model, context\n",
    "from mindspore.train.serialization import load_param_into_net, load_checkpoint\n",
    "from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\n",
    "from mindspore import dataset as ds\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from mindspore import Parameter\n",
    "from mindspore.nn.loss.loss import _Loss\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.common import dtype as mstype"
   ],
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.设置运行环境"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target='Ascend')"
   ],
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.查看数据内容"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#查看训练数据内容前10行内容\n",
    "with open(\"cmn_zhsim.txt\", 'r', encoding='utf-8') as f:\n",
    "        for i in range(10):\n",
    "            print(f.readline())"
   ],
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Hi.\t嗨。\n\nHi.\t你好。\n\nRun.\t你用跑的。\n\nWait!\t等等！\n\nHello!\t你好。\n\nI try.\t让我来。\n\nI won!\t我赢了。\n\nOh no!\t不会吧。\n\nCheers!\t干杯!\n\nHe ran.\t他跑了。\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.定义数据预处理函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "EOS = \"<eos>\"\n",
    "SOS = \"<sos>\"\n",
    "MAX_SEQ_LEN=10"
   ],
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#我们需要将字符转化为ASCII编码\n",
    "#并全部转化为小写字母，并修剪大部分标点符号\n",
    "#除了(a-z, A-Z, \".\", \"?\", \"!\", \",\")这些字符外，全替换成空格\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = unicodeToAscii(s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ],
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_data(data_path, vocab_save_path, max_seq_len):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # 读取文本文件，按行分割，再将每行分割成语句对\n",
    "    data = data.split('\\n')\n",
    "\n",
    "     # 截取前2000行数据进行训练\n",
    "    data = data[:2000]\n",
    "\n",
    "    # 分割每行中的中英文\n",
    "    en_data = [normalizeString(line.split('\\t')[0]) for line in data]\n",
    "\n",
    "    ch_data = [line.split('\\t')[1] for line in data]\n",
    "\n",
    "    # 利用集合，获得中英文词汇表\n",
    "    en_vocab = set(' '.join(en_data).split(' '))\n",
    "    id2en = [EOS] + [SOS] + list(en_vocab)\n",
    "    en2id = {c:i for i,c in enumerate(id2en)}\n",
    "    en_vocab_size = len(id2en)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "\n",
    "    ch_vocab = set(''.join(ch_data))\n",
    "    id2ch = [EOS] + [SOS] + list(ch_vocab)\n",
    "    ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "    ch_vocab_size = len(id2ch)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    # 将句子用词汇表id表示\n",
    "    en_num_data = np.array([[1] + [int(en2id[en]) for en in line.split(' ')] + [0] for line in en_data])\n",
    "    ch_num_data = np.array([[1] + [int(ch2id[ch]) for ch in line] + [0] for line in ch_data])\n",
    "\n",
    "    #将短句子扩充到统一的长度\n",
    "    for i in range(len(en_num_data)):\n",
    "        num = max_seq_len + 1 - len(en_num_data[i])\n",
    "        if(num >= 0):\n",
    "            en_num_data[i] += [0]*num\n",
    "        else:\n",
    "            en_num_data[i] = en_num_data[i][:max_seq_len] + [0]\n",
    "\n",
    "    for i in range(len(ch_num_data)):\n",
    "        num = max_seq_len + 1 - len(ch_num_data[i])\n",
    "        if(num >= 0):\n",
    "            ch_num_data[i] += [0]*num\n",
    "        else:\n",
    "            ch_num_data[i] = ch_num_data[i][:max_seq_len] + [0]\n",
    "    \n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    return en_num_data, ch_num_data, en_vocab_size, ch_vocab_size"
   ],
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.获得mindrecord文件"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#将处理后的数据保存为mindrecord文件，方便后续训练\n",
    "def convert_to_mindrecord(data_path, mindrecord_save_path, max_seq_len):\n",
    "    en_num_data, ch_num_data, en_vocab_size, ch_vocab_size = prepare_data(data_path, mindrecord_save_path, max_seq_len)\n",
    "\n",
    "    # 输出前十行英文句子对应的数据\n",
    "    for i in range(10):\n",
    "        print(en_num_data[i])\n",
    "    \n",
    "    data_list_train = []\n",
    "    for en, ch in zip(en_num_data, ch_num_data):\n",
    "        en = np.array(en).astype(np.int32)\n",
    "        ch = np.array(ch).astype(np.int32)\n",
    "        data_json = {\"encoder_data\": en.reshape(-1),\n",
    "                     \"decoder_data\": ch.reshape(-1)}\n",
    "        data_list_train.append(data_json)\n",
    "    \n",
    "    data_list_eval = random.sample(data_list_train, 20)\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_train.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    schema_json = {\"encoder_data\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"decoder_data\": {\"type\": \"int32\", \"shape\": [-1]}}\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_train)\n",
    "    writer.commit()\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_eval.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_eval)\n",
    "    writer.commit()\n",
    "\n",
    "    # 输出英文词表大小\n",
    "    print(\"en_vocab_size: \", en_vocab_size)\n",
    "    # 输出中文词表大小\n",
    "    print(\"ch_vocab_size: \", ch_vocab_size)\n",
    "\n",
    "    return en_vocab_size, ch_vocab_size"
   ],
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 调用convert_to_mindrecord方法，输出前十行英文句子对应的数据\n",
    "# 注意！！！由于工作空间中已经存在preprocess文件夹及mindrecord文件，第二次运行convert_to_mindrecord方法时会报错。\n",
    "# 此时请运行下面注释中的linux命令删除preprocess文件夹\n",
    "!rm -rf preprocess/\n",
    "if not os.path.exists(\"./preprocess\"):\n",
    "    os.mkdir('./preprocess')\n",
    "convert_to_mindrecord(\"cmn_zhsim.txt\", './preprocess', MAX_SEQ_LEN)"
   ],
   "metadata": {},
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "[1, 957, 38, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 957, 38, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 386, 38, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 861, 806, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 426, 806, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 317, 592, 38, 0, 0, 0, 0, 0, 0, 0]\n[1, 317, 227, 806, 0, 0, 0, 0, 0, 0, 0]\n[1, 163, 1042, 806, 0, 0, 0, 0, 0, 0, 0]\n[1, 200, 806, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 306, 65, 38, 0, 0, 0, 0, 0, 0, 0]\nen_vocab_size:  1154\nch_vocab_size:  1116\n",
     "output_type": "stream"
    },
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1154, 1116)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 运行linux命令，查看当前路径下的文件，可以看到，在工作空间中，存在cmn_zhsim.txt数据集文件，和preprocess文件夹。\n",
    "# 注意！！！preprocess文件夹为当前代码创建，只存在于当前的工作空间，并不在OBS桶中。\n",
    "!ls"
   ],
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "cmn_zhsim_mini.txt  cmn_zhsim.txt  preprocess  seq2seq-main.ipynb\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.超参数设置"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from easydict import EasyDict as edict\n",
    "# hidden_size默认为1024, num_epochs默认为15\n",
    "#注意！！！'en_vocab_size'与'ch_vocab_size'两项，使用cmn_zhsim.txt和cmn_zhsim_mini.txt会有不同的设置，具体值请查看步骤5\n",
    "\n",
    "# CONFIG\n",
    "cfg = edict({\n",
    "    'en_vocab_size': 1154,\n",
    "    'ch_vocab_size': 1116,\n",
    "    'max_seq_length': 10,\n",
    "    'hidden_size': 1024,\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 1,\n",
    "    'learning_rate': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'num_epochs': 15,\n",
    "    'save_checkpoint_steps': 125,\n",
    "    'keep_checkpoint_max': 10,\n",
    "    'dataset_path':'./preprocess',\n",
    "    'ckpt_save_path':'./ckpt',\n",
    "})"
   ],
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.定义读取mindrecord函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def target_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    target_data = decoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data, target_data\n",
    "\n",
    "def eval_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data\n",
    "\n",
    "def create_dataset(data_home, batch_size, repeat_num=1, is_training=True, device_num=1, rank=0):\n",
    "    if is_training:\n",
    "        data_dir = os.path.join(data_home, \"gru_train.mindrecord\")\n",
    "    else:\n",
    "        data_dir = os.path.join(data_home, \"gru_eval.mindrecord\")\n",
    "    data_set = ds.MindDataset(data_dir, columns_list=[\"encoder_data\",\"decoder_data\"], \n",
    "                              num_parallel_workers=4,\n",
    "                              num_shards=device_num, shard_id=rank)\n",
    "    if is_training:\n",
    "        operations = target_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                    output_columns=[\"encoder_data\",\"decoder_data\",\"target_data\"],\n",
    "                    column_order=[\"encoder_data\",\"decoder_data\",\"target_data\"])\n",
    "    else:\n",
    "        operations = eval_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                   output_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                   column_order=[\"encoder_data\",\"decoder_data\"])\n",
    "    data_set = data_set.shuffle(buffer_size=data_set.get_dataset_size())\n",
    "    data_set = data_set.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    data_set = data_set.repeat(count=repeat_num)\n",
    "    return data_set"
   ],
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ds_train = create_dataset(cfg.dataset_path, cfg.batch_size)"
   ],
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.定义GRU单元"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def gru_default_state(batch_size, input_size, hidden_size, num_layers=1, bidirectional=False):\n",
    "    '''Weight init for gru cell'''\n",
    "    stdv = 1 / math.sqrt(hidden_size)\n",
    "    weight_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (input_size, 3*hidden_size)).astype(np.float32)), \n",
    "                         name='weight_i')\n",
    "    weight_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (hidden_size, 3*hidden_size)).astype(np.float32)), \n",
    "                         name='weight_h')\n",
    "    bias_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_i')\n",
    "    bias_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_h')\n",
    "    return weight_i, weight_h, bias_i, bias_h\n",
    "\n",
    "'''\n",
    "定义GRU单元，查阅官方文档，了解P.DynamicGRUV2()的输入输出组成与计算原理，并写入实验报告。\n",
    "'''\n",
    "class GRU(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(GRU, self).__init__()\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.weight_i, self.weight_h, self.bias_i, self.bias_h = \\\n",
    "            gru_default_state(self.batch_size, self.hidden_size, self.hidden_size)\n",
    "        self.rnn = P.DynamicGRUV2()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        x = self.cast(x, mstype.float16)\n",
    "        y1, h1, _, _, _, _ = self.rnn(x, self.weight_i, self.weight_h, self.bias_i, self.bias_h, None, hidden)\n",
    "        return y1, h1"
   ],
   "metadata": {},
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.定义Encoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = config.en_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n",
    "\n",
    "    def construct(self, encoder_input):\n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.embedding，对输入encoder_input进行Embedding编码。\n",
    "        并根据self.perm定义的维度，通过self.trans进行维度变换。\n",
    "\n",
    "\n",
    "        将处理得到的输出命名为embeddings\n",
    "        '''\n",
    "        \n",
    "        temp_embeddings = self.embedding(encoder_input)\n",
    "        embeddings = self.trans(temp_embeddings, self.perm)\n",
    "        \n",
    "        output, hidden = self.gru(embeddings, self.h)\n",
    "        return output, hidden"
   ],
   "metadata": {},
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10.定义Decoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = config.ch_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_len = config.max_seq_length\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(1-dropout)\n",
    "        self.attn = nn.Dense(self.hidden_size, self.max_len)\n",
    "        self.softmax = nn.Softmax(axis=2)\n",
    "        self.bmm = P.BatchMatMul()\n",
    "        self.concat = P.Concat(axis=2)\n",
    "        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(axis=2)\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, decoder_input, hidden, encoder_output):\n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.embedding，对输入decoder_input进行Embedding编码。\n",
    "        并对其进行dropout操作\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        将处理得到的输出命名为embeddings\n",
    "        '''\n",
    "        \n",
    "        embeddings = self.embedding(decoder_input)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        # calculate attn\n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.attn，对embeddings计算注意力权重。\n",
    "        并使用softmax处理注意力权重\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        将处理得到的输出命名为attn_weights\n",
    "        '''\n",
    "        \n",
    "        attn_weights = self.attn(embeddings)\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "        \n",
    "        encoder_output = self.trans(encoder_output, self.perm)\n",
    "        attn_applied = self.bmm(attn_weights, self.cast(encoder_output,mstype.float32))\n",
    "        output =  self.concat((embeddings, attn_applied))\n",
    "        output = self.attn_combine(output)\n",
    "\n",
    "\n",
    "        embeddings = self.trans(embeddings, self.perm)\n",
    "        output, hidden = self.gru(embeddings, hidden)\n",
    "        output = self.cast(output, mstype.float32)\n",
    "        \n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.out，处理上步得到的output，将特征维度映射到词表大小。\n",
    "        并使用self.logsoftmax处理输出\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        将处理得到的输出继续命名为output\n",
    "        '''\n",
    "        \n",
    "        output = self.out(output)\n",
    "        output = self.logsoftmax(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ],
   "metadata": {},
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.定义Seq2Seq整体结构"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, config, is_train=True):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.encoder = Encoder(config, is_train)\n",
    "        self.decoder = Decoder(config, is_train)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.squeeze = P.Squeeze(axis=0)\n",
    "        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n",
    "        self.concat = P.Concat(axis=1)\n",
    "        self.concat2 = P.Concat(axis=0)\n",
    "        self.select = P.Select()\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        decoder_hidden = self.squeeze(encoder_output[self.max_len-2:self.max_len-1:1, ::, ::])\n",
    "        if self.is_train:\n",
    "            outputs, _ = self.decoder(dst, decoder_hidden, encoder_output)\n",
    "        else:\n",
    "            decoder_input = dst[::,0:1:1]\n",
    "            decoder_outputs = ()\n",
    "            for i in range(0, self.max_len):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n",
    "                                                                 decoder_hidden, encoder_output)\n",
    "                decoder_hidden = self.squeeze(decoder_hidden)\n",
    "                decoder_output, _ = self.argmax(decoder_output)\n",
    "                decoder_output = self.squeeze(decoder_output)\n",
    "                decoder_outputs += (decoder_output,)\n",
    "                decoder_input = decoder_output\n",
    "            outputs = self.concat(decoder_outputs)\n",
    "        return outputs"
   ],
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 12.定义损失函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class NLLLoss(_Loss):\n",
    "    '''\n",
    "       NLLLoss function\n",
    "    '''\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NLLLoss, self).__init__(reduction)\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n",
    "                                     F.scalar_to_array(0.0))\n",
    "        #print('NLLLoss label_one_hot:',label_one_hot, label_one_hot.shape)\n",
    "        #print('NLLLoss logits:',logits, logits.shape)\n",
    "        #print('xxx:', logits * label_one_hot)\n",
    "        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n",
    "        return self.get_loss(loss)\n",
    "    \n",
    "class WithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, config):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self.batch_size = config.batch_size\n",
    "        self.onehot = nn.OneHot(depth=config.ch_vocab_size)\n",
    "        self._loss_fn = NLLLoss()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.squeeze = P.Squeeze()\n",
    "        self.cast = P.Cast()\n",
    "        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n",
    "        self.print = P.Print()\n",
    "\n",
    "    def construct(self, src, dst, label):\n",
    "        out = self._backbone(src, dst)\n",
    "        loss_total = 0\n",
    "        for i in range(self.batch_size):\n",
    "            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n",
    "                                 self.squeeze(label[i:i+1:1, ::]))\n",
    "            loss_total += loss\n",
    "        loss = loss_total / self.batch_size\n",
    "        return loss"
   ],
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13.定义训练网络与优化器"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "network = Seq2Seq(cfg)\n",
    "network = WithLossCell(network, cfg)\n",
    "optimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\n",
    "model = Model(network, optimizer=optimizer)"
   ],
   "metadata": {},
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "text": "[WARNING] ME(1969:281473159085664,MainProcess):2022-12-13-08:41:30.257.317 [mindspore/nn/loss/loss.py:103] '_Loss' is deprecated from version 1.3 and will be removed in a future version, use 'LossBase' instead.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14. 定义回调函数，构建模型，启动训练"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "loss_cb = LossMonitor()\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps, keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"gru\", directory=cfg.ckpt_save_path, config=config_ck)\n",
    "time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
    "callbacks = [time_cb, ckpoint_cb, loss_cb]\n",
    "model.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=True)"
   ],
   "metadata": {},
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": "epoch: 1 step: 125, loss is 2.6524706\nepoch time: 75375.872 ms, per step time: 603.007 ms\nepoch: 2 step: 125, loss is 2.5780191\nepoch time: 6301.599 ms, per step time: 50.413 ms\nepoch: 3 step: 125, loss is 2.0820508\nepoch time: 6302.922 ms, per step time: 50.423 ms\nepoch: 4 step: 125, loss is 1.4764645\nepoch time: 6299.948 ms, per step time: 50.400 ms\nepoch: 5 step: 125, loss is 1.5869874\nepoch time: 6300.489 ms, per step time: 50.404 ms\nepoch: 6 step: 125, loss is 0.8512421\nepoch time: 6303.458 ms, per step time: 50.428 ms\nepoch: 7 step: 125, loss is 0.83606255\nepoch time: 6300.667 ms, per step time: 50.405 ms\nepoch: 8 step: 125, loss is 0.58129364\nepoch time: 6300.169 ms, per step time: 50.401 ms\nepoch: 9 step: 125, loss is 0.5085277\nepoch time: 6302.872 ms, per step time: 50.423 ms\nepoch: 10 step: 125, loss is 0.28547084\nepoch time: 6299.776 ms, per step time: 50.398 ms\nepoch: 11 step: 125, loss is 0.15103929\nepoch time: 6326.054 ms, per step time: 50.608 ms\nepoch: 12 step: 125, loss is 0.14183462\nepoch time: 6326.132 ms, per step time: 50.609 ms\nepoch: 13 step: 125, loss is 0.0825462\nepoch time: 6326.655 ms, per step time: 50.613 ms\nepoch: 14 step: 125, loss is 0.0741814\nepoch time: 6325.818 ms, per step time: 50.607 ms\nepoch: 15 step: 125, loss is 0.11613748\nepoch time: 6325.465 ms, per step time: 50.604 ms\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 16.定义推理网络"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class InferCell(nn.Cell):\n",
    "    def __init__(self, network, config):\n",
    "        super(InferCell, self).__init__(auto_prefix=False)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        out = self.network(src, dst)\n",
    "        return out"
   ],
   "metadata": {},
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "network = Seq2Seq(cfg,is_train=False)\n",
    "network = InferCell(network, cfg)\n",
    "network.set_train(False)\n",
    "# 注意，不同的checkpoint请自行设置\n",
    "parameter_dict = load_checkpoint(\"./ckpt/gru-15_125.ckpt\")\n",
    "load_param_into_net(network, parameter_dict)\n",
    "model = Model(network)"
   ],
   "metadata": {},
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 17.定义翻译测试函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#打开词汇表\n",
    "with open(os.path.join(cfg.dataset_path,\"en_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "en_vocab = list(data.split('\\n'))\n",
    "\n",
    "with open(os.path.join(cfg.dataset_path,\"ch_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "ch_vocab = list(data.split('\\n'))\n",
    "\n",
    "def translate(str_en):\n",
    "    max_seq_len = 10\n",
    "    str_vocab = normalizeString(str_en).split(' ')\n",
    "    print(\"English\",str(str_vocab))\n",
    "    str_id = [1]\n",
    "    for i in str_vocab:\n",
    "        str_id += [en_vocab.index(i)]\n",
    "   \n",
    "    num = max_seq_len + 1 - len(str_id)\n",
    "    if(num >= 0):\n",
    "        str_id += [0]*num\n",
    "    else:\n",
    "        str_id = str_id[:max_seq_len] + [0]\n",
    "    str_id = Tensor(np.array([str_id[1:]]).astype(np.int32))\n",
    "   \n",
    "    out_id = [1]+[0]*10\n",
    "    out_id = Tensor(np.array([out_id[:-1]]).astype(np.int32))\n",
    "    \n",
    "    output = network(str_id, out_id)\n",
    "    out= ''\n",
    "    for x in output[0].asnumpy():\n",
    "        if x == 0:\n",
    "            break\n",
    "        out += ch_vocab[x]\n",
    "    print(\"中文\",out)"
   ],
   "metadata": {},
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18.在线推理测试"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "translate('i love tom')"
   ],
   "metadata": {},
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "English ['i', 'love', 'tom']\n中文 我爱汤姆。\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "translate('i hate tom')"
   ],
   "metadata": {},
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "English ['i', 'hate', 'tom']\n中文 我恨汤姆。\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "translate('Hi')"
   ],
   "metadata": {},
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": "English ['hi']\n中文 你好。\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
